{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create QNetwork Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'RLCW' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n RLCW ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_size,\n",
    "        buffer_size,\n",
    "        batch_size,\n",
    "        seed,\n",
    "        device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    ):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\n",
    "            \"Experience\",\n",
    "            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"],\n",
    "        )\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = (\n",
    "            torch.from_numpy(np.vstack([e.state for e in experiences if e is not None]))\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        actions = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.action for e in experiences if e is not None])\n",
    "            )\n",
    "            .long()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        rewards = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.reward for e in experiences if e is not None])\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        next_states = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.next_state for e in experiences if e is not None])\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        dones = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.done for e in experiences if e is not None]).astype(\n",
    "                    np.uint8\n",
    "                )\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Utility Functions to Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import torch\n",
    "\n",
    "def train_agent(\n",
    "    agent,\n",
    "    env,\n",
    "    agent_type,\n",
    "    n_episodes=1000,\n",
    "    max_t=1000,\n",
    "    eps_start=1.0,\n",
    "    eps_end=0.01,\n",
    "    save_agent=False,\n",
    "):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    exploitative_actions = []\n",
    "    exploratory_actions = []\n",
    "\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start  # initialize epsilon\n",
    "    eps_change = [eps]\n",
    "\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        episode_length = 0\n",
    "        for _ in range(max_t):\n",
    "\n",
    "            # Increment the episode length counter\n",
    "            episode_length += 1\n",
    "\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated | truncated\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # save total loss during the episode and reset it\n",
    "        losses.append(agent.loss)\n",
    "        agent.loss = 0\n",
    "\n",
    "        exploitative_actions.append(agent.num_exploitative_actions)\n",
    "        agent.num_exploitative_actions = 0\n",
    "\n",
    "        exploratory_actions.append(agent.num_exploratory_actions)\n",
    "        agent.num_exploratory_actions = 0\n",
    "\n",
    "        episode_lengths.append(episode_length)\n",
    "\n",
    "        scores_window.append(score)  # save most recent score\n",
    "        scores.append(score)  # save most recent score\n",
    "\n",
    "        eps = max(eps_end, agent.eps_decay * eps)  # decrease epsilon\n",
    "        eps_change.append(eps)\n",
    "        print(\n",
    "            \"\\rEpisode {}\\tAverage Score: {:.2f}\".format(\n",
    "                i_episode, np.mean(scores_window)\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\n",
    "                \"\\rEpisode {}\\tAverage Score: {:.2f}\".format(\n",
    "                    i_episode, np.mean(scores_window)\n",
    "                )\n",
    "            )\n",
    "        if np.mean(scores_window) >= 200.0:\n",
    "            print(\n",
    "                \"\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}\".format(\n",
    "                    i_episode - 100, np.mean(scores_window)\n",
    "                )\n",
    "            )\n",
    "            if save_agent:\n",
    "                torch.save(\n",
    "                    agent.target_network.state_dict(),\n",
    "                    f\"checkpoints/{agent_type}_target_network_{i_episode}.pth\",\n",
    "                )\n",
    "                torch.save(\n",
    "                    agent.online_network.state_dict(),\n",
    "                    f\"checkpoints/{agent_type}_online_network_{i_episode}.pth\",\n",
    "                )\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"episode_lengths\": episode_lengths,\n",
    "        \"losses\": losses,\n",
    "        \"exploitative_actions\": exploitative_actions,\n",
    "        \"exploratory_actions\": exploratory_actions,\n",
    "        \"eps_change\": eps_change,\n",
    "    }\n",
    "    \n",
    "def get_optimal_hyperparamters(env, agent_type, n_trials=10, n_episodes=1000, seed=42):\n",
    "    def objective(trial):\n",
    "\n",
    "        # Sample hyperparameter values\n",
    "        buffer_size = trial.suggest_int(\"buffer_size\", 1000, 10000)\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 32, 256)\n",
    "        fc1_units = trial.suggest_int(\"fc1_units\", 16, 128)\n",
    "        fc2_units = trial.suggest_int(\"fc2_units\", 16, 128)\n",
    "        gamma = trial.suggest_float(\"gamma\", 0.9, 1.0)\n",
    "        tau = trial.suggest_float(\"tau\", 1e-5, 1e-3)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "        update_every = trial.suggest_int(\"update_every\", 1, 6)\n",
    "        eps_decay = trial.suggest_float(\"eps_decay\", 0.9, 0.999)\n",
    "        loss_fn = trial.suggest_categorical(\"loss_fn\", [\"mse\", \"huber\"])\n",
    "\n",
    "        # Create and train DDQN agent\n",
    "        agent = (\n",
    "            DDQNAgent(\n",
    "                state_size=8,\n",
    "                action_size=4,\n",
    "                seed=seed,\n",
    "                buffer_size=buffer_size,\n",
    "                batch_size=batch_size,\n",
    "                fc1_units=fc1_units,\n",
    "                fc2_units=fc2_units,\n",
    "                eps_decay=eps_decay,\n",
    "                gamma=gamma,\n",
    "                tau=tau,\n",
    "                lr=lr,\n",
    "                update_every=update_every,\n",
    "                loss_fn=loss_fn,\n",
    "            )\n",
    "            if agent_type == \"ddqn\"\n",
    "            else DQNAgent(\n",
    "                state_size=8,\n",
    "                action_size=4,\n",
    "                seed=seed,\n",
    "                buffer_size=buffer_size,\n",
    "                batch_size=batch_size,\n",
    "                fc1_units=fc1_units,\n",
    "                fc2_units=fc2_units,\n",
    "                eps_decay=eps_decay,\n",
    "                gamma=gamma,\n",
    "                tau=tau,\n",
    "                lr=lr,\n",
    "                update_every=update_every,\n",
    "            )\n",
    "        )\n",
    "        metrics = train_agent(agent, env, n_episodes=n_episodes, agent_type=agent_type)\n",
    "        # Return average reward over all episodes\n",
    "        return np.mean(metrics[\"scores\"])\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"{agent_type}_study\",\n",
    "        direction=\"maximize\",\n",
    "        sampler=TPESampler(seed=seed),\n",
    "        storage=None,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    optuna.visualization.plot_optimization_history(study)\n",
    "\n",
    "    optuna.visualization.plot_slice(study)\n",
    "\n",
    "    optuna.visualization.plot_param_importances(study)\n",
    "   \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 5\n",
    "n_episodes = 500\n",
    "agent_type = \"ddqn\"\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "opt_params = get_optimal_hyperparamters(\n",
    "        env, n_trials=n_trials, n_episodes=n_episodes, agent_type=agent_type\n",
    ")\n",
    "\n",
    "print(opt_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLCW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Oct 24 2022, 11:04:34) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f88920a339849176b91cba49f41b5bf4abbaa564804cdd9b66793d193bef424"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
