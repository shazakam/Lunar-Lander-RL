{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create QNetwork Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDQN Agent in Lunar Lander Version 2\n",
    "\n",
    "To run this code safely do it in google colab and comment out anything that saves plots or agent's networks to google drive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create QNetwork Class\n",
    "\n",
    "Here we create a QNetwork class that will be used for the agents target and online network, it also takes into account optional layers that will added during hyperparameter tuning later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'RLCW' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n RLCW ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64, fc3_units=0, fc4_units=0):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "            fc3_units (int): Number of nodes in third hidden layer (optional)\n",
    "            fc4_units (int): Number of nodes in fourth hidden layer (optional)\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        if fc3_units != 0:\n",
    "            self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        if fc4_units != 0:\n",
    "            self.fc4 = nn.Linear(fc3_units if fc3_units != 0 else fc2_units, fc4_units)\n",
    "        self.fc5 = nn.Linear((fc4_units if fc4_units != 0 else fc3_units if fc3_units != 0 else fc2_units), action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if hasattr(self, \"fc3\"):\n",
    "            x = F.relu(self.fc3(x))\n",
    "        if hasattr(self, \"fc4\"):\n",
    "            x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "\n",
    "# TPU torch.device(xm.xla_device() if xm.xla_available()\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_size,\n",
    "        buffer_size,\n",
    "        batch_size,\n",
    "        seed,\n",
    "        device=torch.device(torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\")\n",
    "    ):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\n",
    "            \"Experience\",\n",
    "            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"],\n",
    "        )\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = (\n",
    "            torch.from_numpy(np.vstack([e.state for e in experiences if e is not None]))\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        actions = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.action for e in experiences if e is not None])\n",
    "            )\n",
    "            .long()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        rewards = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.reward for e in experiences if e is not None])\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        next_states = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.next_state for e in experiences if e is not None])\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        dones = (\n",
    "            torch.from_numpy(\n",
    "                np.vstack([e.done for e in experiences if e is not None]).astype(\n",
    "                    np.uint8\n",
    "                )\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gymnasium\n",
    "!{sys.executable} -m pip install optuna\n",
    "!{sys.executable} -m pip install gymnasium[box2d]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DDQN Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        seed,\n",
    "        loss_fn,\n",
    "        buffer_size=10000,\n",
    "        batch_size=64,\n",
    "        fc1_units=64,\n",
    "        fc2_units=64,\n",
    "        fc3_units=0,\n",
    "        fc4_units=0,\n",
    "        gamma=0.99,\n",
    "        tau=1e-3,\n",
    "        lr=5e-4,\n",
    "        update_every=4,\n",
    "        # this is used in the training loop but we want to see how a change in this can affect traning so need to be here for optune optimizer\n",
    "        eps_decay=0.995,\n",
    "        device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    ):\n",
    "        self.num_exploitative_actions = 0\n",
    "        self.loss_fn = loss_fn\n",
    "        self.eps_decay = eps_decay\n",
    "        self.num_exploratory_actions = 0\n",
    "        self.loss = 0.0\n",
    "        self.device = device\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.lr = lr\n",
    "        self.update_every = update_every\n",
    "        self.steps_done = 0\n",
    "        self.online_network = QNetwork(\n",
    "            state_size, action_size, self.seed, fc1_units, fc2_units, fc3_units=fc3_units, fc4_units=fc4_units\n",
    "        ).to(self.device)\n",
    "        self.target_network = QNetwork(\n",
    "            state_size, action_size, self.seed, fc1_units, fc2_units, fc3_units=fc3_units, fc4_units=fc4_units\n",
    "        ).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.online_network.parameters(), lr=self.lr)\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, self.seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.update_counter = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.update_counter = (self.update_counter + 1) % self.update_every\n",
    "        if self.update_counter == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "    def act(self, state, eps=0.0):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.online_network.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.online_network(state)\n",
    "        self.online_network.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            self.num_exploratory_actions += 1\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            self.num_exploitative_actions += 1\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from online model\n",
    "        q_online = self.online_network(next_states).detach()\n",
    "        best_actions = torch.argmax(q_online, dim=1)\n",
    "        q_target = self.target_network(next_states).detach()\n",
    "        Q_targets_next = q_target[range(self.batch_size), best_actions]\n",
    "        Q_targets_next = Q_targets_next.unsqueeze(1)\n",
    "\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.online_network(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss - only two options and if it is not mse it is huber loss\n",
    "        loss = (\n",
    "            F.mse_loss(Q_expected, Q_targets)\n",
    "            if self.loss_fn == \"mse\"\n",
    "            else F.huber_loss(Q_expected, Q_targets)\n",
    "        )\n",
    "        self.loss += loss.item()\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update the target network to have the same weights as the online network.\"\"\"\n",
    "        for target_param, online_param in zip(\n",
    "            self.target_network.parameters(), self.online_network.parameters()\n",
    "        ):\n",
    "            target_param.data.copy_(\n",
    "                self.tau * online_param.data + (1.0 - self.tau) * target_param.data\n",
    "            )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Utility Functions to Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from google.colab import drive\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "def train_agent(\n",
    "    agent,\n",
    "    env,\n",
    "    agent_type,\n",
    "    n_episodes=1000,\n",
    "    max_t=1000,\n",
    "    eps_start=1.0,\n",
    "    eps_end=0.01,\n",
    "    save_agent=False,\n",
    "    save_location='/content/drive/My Drive/RLCW',\n",
    "    trial=None\n",
    "):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    exploitative_actions = []\n",
    "    exploratory_actions = []\n",
    "\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start  # initialize epsilon\n",
    "    eps_change = [eps]\n",
    "\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        episode_length = 0\n",
    "        for _ in range(max_t):\n",
    "\n",
    "            # Increment the episode length counter\n",
    "            episode_length += 1\n",
    "\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated | truncated\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if trial:\n",
    "            # Report the score to Optuna\n",
    "            trial.report(score, step=i_episode)\n",
    "            # Check if the trial should be pruned\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned() \n",
    "\n",
    "        # save total loss during the episode and reset it\n",
    "        losses.append(agent.loss)\n",
    "        agent.loss = 0\n",
    "\n",
    "        exploitative_actions.append(agent.num_exploitative_actions)\n",
    "        agent.num_exploitative_actions = 0\n",
    "\n",
    "        exploratory_actions.append(agent.num_exploratory_actions)\n",
    "        agent.num_exploratory_actions = 0\n",
    "\n",
    "        episode_lengths.append(episode_length)\n",
    "\n",
    "        scores_window.append(score)  # save most recent score\n",
    "        scores.append(score)  # save most recent score\n",
    "\n",
    "        eps = max(eps_end, agent.eps_decay * eps)  # decrease epsilon\n",
    "        eps_change.append(eps)\n",
    "        print(\n",
    "            \"\\rEpisode {}\\tAverage Score: {:.2f}\".format(\n",
    "                i_episode, np.mean(scores_window)\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        if i_episode % 100 == 0:\n",
    "            if save_agent:\n",
    "                torch.save(agent.target_network.state_dict(), f'{save_location}/{agent_type}/models/{agent_type}_target_network_{i_episode}.pth')\n",
    "                torch.save(agent.online_network.state_dict(), f'{save_location}/{agent_type}/models/{agent_type}_online_network_{i_episode}.pth')\n",
    "            print(\n",
    "                \"\\rEpisode {}\\tAverage Score: {:.2f}\".format(\n",
    "                    i_episode, np.mean(scores_window)\n",
    "                )\n",
    "            )\n",
    "        if np.mean(scores_window) >= 200.0:\n",
    "            print(\n",
    "                \"\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}\".format(\n",
    "                    i_episode , np.mean(scores_window)\n",
    "                )\n",
    "            )\n",
    "            if save_agent:\n",
    "                torch.save(agent.target_network.state_dict(), f'{save_location}/{agent_type}/models/{agent_type}_target_network_{i_episode}.pth')\n",
    "                torch.save(agent.online_network.state_dict(), f'{save_location}/{agent_type}/models/{agent_type}_online_network_{i_episode}.pth')\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"episode_lengths\": episode_lengths,\n",
    "        \"losses\": losses,\n",
    "        \"exploitative_actions\": exploitative_actions,\n",
    "        \"exploratory_actions\": exploratory_actions,\n",
    "        \"eps_change\": eps_change,\n",
    "    }\n",
    "    \n",
    "def get_optimal_hyperparamters(env, n_trials=30, n_episodes=1000, seed=42, save_location='/content/drive/My Drive/RLCW', agent_type='ddqn'):\n",
    "    def objective(trial):\n",
    "        # Sample hyperparameter values\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 32, 256)\n",
    "        fc1_units = trial.suggest_int(\"fc1_units\", 16, 128)\n",
    "        fc2_units = trial.suggest_int(\"fc2_units\", 16, 128)\n",
    "\n",
    "        optional_layer_1 = trial.suggest_categorical(\"include_optional_layer_1\", [True, False])\n",
    "        optional_layer_2 = trial.suggest_categorical(\"include_optional_layer_2\", [True, False])\n",
    "\n",
    "        fc3_units = 0\n",
    "        if optional_layer_1:\n",
    "            fc3_units = trial.suggest_int(\"fc3_units\", 16, 128) \n",
    "\n",
    "        fc4_units = 0\n",
    "        if optional_layer_2:\n",
    "            fc4_units = trial.suggest_int(\"fc4_units\", 16, 128) \n",
    "\n",
    "        gamma = trial.suggest_float(\"gamma\", 0.9, 1.0)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "        loss_fn = trial.suggest_categorical(\"loss_fn\", [\"mse\", \"huber\"])\n",
    "        \n",
    "        # Create and train DDQN agent\n",
    "        agent = (\n",
    "            DDQNAgent(\n",
    "                state_size=8,\n",
    "                action_size=4,\n",
    "                seed=seed,\n",
    "                batch_size=batch_size,\n",
    "                fc1_units=fc1_units,\n",
    "                fc2_units=fc2_units,\n",
    "                fc3_units=fc3_units,\n",
    "                fc4_units=fc4_units,\n",
    "                gamma=gamma,\n",
    "                lr=lr,\n",
    "                loss_fn=loss_fn,\n",
    "            )\n",
    "        )\n",
    "        metrics = train_agent(agent, env, n_episodes=n_episodes, agent_type=agent_type, trial=trial)\n",
    "        # Return average reward over all episodes\n",
    "        return np.mean(metrics[\"scores\"])\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"{agent_type}_study\",\n",
    "        direction=\"maximize\",\n",
    "        sampler=TPESampler(seed=seed),\n",
    "        pruner=MedianPruner(n_warmup_steps=140, interval_steps=20, n_startup_trials=5), \n",
    "        storage=None,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "    # Comment out line below if you want to run and not save to drive\n",
    "    plt.savefig(f\"{save_location}/{agent_type}/graphs/hyperparameters/{agent_type}_optimization_history.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    optuna.visualization.matplotlib.plot_slice(study)\n",
    "    # Comment out line below if you want to run and not save to drive\n",
    "    plt.savefig(f\"{save_location}/{agent_type}/graphs/hyperparameters/{agent_type}_plot_slice.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "    # Comment out line below if you want to run and not save to drive\n",
    "    plt.savefig(f\"{save_location}/{agent_type}/graphs/hyperparameters/{agent_type}_paramter_importance.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    return study.best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tuning\n",
    "\n",
    " Here we get attempt to explore the optimal hyperparameters to train the agent with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "n_trials = 35\n",
    "n_episodes = 500\n",
    "agent_type = \"ddqn\"\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "opt_params = get_optimal_hyperparamters(\n",
    "        env, n_trials=n_trials, n_episodes=n_episodes, agent_type=agent_type\n",
    ")\n",
    "\n",
    "print(\"Optimal Parameters: \", opt_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent With Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "opt_agent = DDQNAgent(\n",
    "                state_size=8,\n",
    "                action_size=4,\n",
    "                seed=42,\n",
    "                fc1_units=64,\n",
    "                fc2_units=64,\n",
    "                fc3_units=64,\n",
    "                fc4_units=64,\n",
    "                #fc3_units= 0 if not opt_params['include_optional_layer_1'] else opt_params['fc3_units'],\n",
    "                #fc4_units= 0 if not opt_params['include_optional_layer_2'] else opt_params['fc4_units'],\n",
    "                #gamma=opt_params['gamma'],\n",
    "                #batch_size=opt_params[\"batch_size\"],\n",
    "                #lr=opt_params[\"lr\"],\n",
    "                #loss_fn=opt_params[\"loss_fn\"],\n",
    "                loss_fn=\"mse\"\n",
    "            )\n",
    "\n",
    "opt_agent2 = DDQNAgent(\n",
    "                state_size=8,\n",
    "                action_size=4,\n",
    "                seed=42,\n",
    "                fc1_units=89,\n",
    "                fc2_units=55,\n",
    "                #fc3_units=64,\n",
    "                #fc4_units=64,\n",
    "                #fc3_units= 0 if not opt_params['include_optional_layer_1'] else opt_params['fc3_units'],\n",
    "                #fc4_units= 0 if not opt_params['include_optional_layer_2'] else opt_params['fc4_units'],\n",
    "                gamma=0.9992470815704736,\n",
    "                batch_size=98,\n",
    "                lr=0.004124018930586374,\n",
    "                loss_fn=\"huber\",\n",
    "            )\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# {'batch_size': 98, 'fc1_units': 89, 'fc2_units': 55, 'include_optional_layer_1': False, 'include_optional_layer_2': False, 'gamma': 0.9992470815704736, 'lr': 0.004124018930586374, 'loss_fn': 'huber'}\n",
    "\n",
    "# if you do not want to save the agent to your drive make save_agent=False\n",
    "metrics = train_agent(env=env, agent=opt_agent2, agent_type=\"ddqn\", save_agent=False, n_episodes=500)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Metric Plots \n",
    "\n",
    "Here we create and save plots that track metrics during training of the optimal agent for analysis purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metric_plots_to_drive(metrics, location='/content/drive/My Drive/RLCW', agent_type='ddqn'):\n",
    "    title = lambda x: x.replace(\"_\", \" \").title()\n",
    "    # used to compute the average score for each 100 eps for a cleaner graph\n",
    "    avg_scores_100 = []\n",
    "    for key, metric in metrics.items():\n",
    "        if key == \"scores\":\n",
    "            for i in range(0, len(metric), 100):\n",
    "                avg_scores_100.append(np.mean(metric[i : i + 100]))\n",
    "        # clear plot so others don't get saved in same img\n",
    "        plt.clf()\n",
    "        key_title = title(key)\n",
    "        plt.plot(avg_scores_100 if key == \"scores\" else metric)\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(key_title)\n",
    "        plt.title(key_title + \" over time\")\n",
    "        plt.savefig(f\"{location}/{agent_type}/graphs/metrics/{agent_type}_{key}\", bbox_inches=\"tight\")\n",
    "        # clear plot so others don't get saved in same img\n",
    "        plt.clf()\n",
    "\n",
    "# Only run this if you want to save metric plots to your drive   \n",
    "save_metric_plots_to_drive(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Googl Colab cannot render the environment so if you want to reproduce creating videos from saved optimal agent run the following file agent_videos.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLCW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Oct 24 2022, 11:04:34) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f88920a339849176b91cba49f41b5bf4abbaa564804cdd9b66793d193bef424"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
